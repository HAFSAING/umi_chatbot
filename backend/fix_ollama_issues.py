#!/usr/bin/env python3
"""
Script de r√©paration rapide pour les probl√®mes Ollama + LLaVA
Place ce fichier dans le dossier backend/ et lance-le
"""

import subprocess
import time
import requests
import sys
import os

def print_step(message, status="INFO"):
    symbols = {"INFO": "‚ÑπÔ∏è", "OK": "‚úÖ", "ERROR": "‚ùå", "WARNING": "‚ö†Ô∏è"}
    print(f"{symbols.get(status)} {message}")

def run_command(cmd, description):
    """Ex√©cute une commande et retourne le r√©sultat"""
    print_step(f"{description}...", "INFO")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            print_step(f"{description} - Succ√®s", "OK")
            return True, result.stdout
        else:
            print_step(f"{description} - √âchec: {result.stderr}", "ERROR")
            return False, result.stderr
    except subprocess.TimeoutExpired:
        print_step(f"{description} - Timeout", "ERROR")
        return False, "Timeout"
    except Exception as e:
        print_step(f"{description} - Erreur: {e}", "ERROR")
        return False, str(e)

def check_ollama_process():
    """V√©rifie si Ollama tourne"""
    print_step("V√©rification du processus Ollama", "INFO")
    
    try:
        result = subprocess.run("pgrep -f ollama", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            print_step(f"Processus Ollama trouv√©s: {len(pids)} (PIDs: {', '.join(pids)})", "OK")
            return True
        else:
            print_step("Aucun processus Ollama trouv√©", "WARNING")
            return False
    except Exception as e:
        print_step(f"Erreur v√©rification processus: {e}", "ERROR")
        return False

def kill_ollama():
    """Tue tous les processus Ollama"""
    print_step("Arr√™t de tous les processus Ollama", "INFO")
    
    commands = [
        "pkill -f ollama",
        "killall ollama 2>/dev/null || true"
    ]
    
    for cmd in commands:
        subprocess.run(cmd, shell=True, capture_output=True)
    
    time.sleep(2)
    
    # V√©rifier si les processus sont bien arr√™t√©s
    if not check_ollama_process():
        print_step("Processus Ollama arr√™t√©s", "OK")
        return True
    else:
        print_step("Certains processus Ollama persistent", "WARNING")
        return False

def start_ollama():
    """D√©marre Ollama en arri√®re-plan"""
    print_step("D√©marrage d'Ollama", "INFO")
    
    try:
        # D√©marrer Ollama en arri√®re-plan
        subprocess.Popen(
            "ollama serve",
            shell=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        
        # Attendre un peu pour que le service d√©marre
        print_step("Attente du d√©marrage du service (10s)", "INFO")
        time.sleep(10)
        
        # V√©rifier que le service r√©pond
        for attempt in range(5):
            try:
                response = requests.get("http://localhost:11434/api/version", timeout=3)
                if response.status_code == 200:
                    version_info = response.json()
                    print_step(f"Ollama d√©marr√© - Version: {version_info.get('version', 'N/A')}", "OK")
                    return True
            except:
                if attempt < 4:
                    print_step(f"Tentative {attempt + 1}/5 - Service pas encore pr√™t", "INFO")
                    time.sleep(3)
                else:
                    print_step("Service Ollama ne r√©pond pas apr√®s 25s", "ERROR")
                    return False
        
        return False
        
    except Exception as e:
        print_step(f"Erreur d√©marrage Ollama: {e}", "ERROR")
        return False

def check_llava_model():
    """V√©rifie si le mod√®le LLaVA est install√©"""
    print_step("V√©rification du mod√®le LLaVA", "INFO")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            model_names = [m['name'] for m in models]
            
            llava_models = [name for name in model_names if 'llava' in name.lower()]
            
            if llava_models:
                print_step(f"Mod√®les LLaVA trouv√©s: {llava_models}", "OK")
                return True, llava_models
            else:
                print_step("Aucun mod√®le LLaVA trouv√©", "ERROR")
                return False, []
        else:
            print_step(f"Erreur API tags: HTTP {response.status_code}", "ERROR")
            return False, []
            
    except Exception as e:
        print_step(f"Erreur v√©rification mod√®les: {e}", "ERROR")
        return False, []

def install_llava():
    """Installe le mod√®le LLaVA"""
    print_step("Installation du mod√®le LLaVA (cela peut prendre plusieurs minutes)", "INFO")
    
    models_to_try = [
        "llava:latest",
        "llava:7b",
        "llava:13b"
    ]
    
    for model in models_to_try:
        print_step(f"Tentative d'installation: {model}", "INFO")
        success, output = run_command(f"ollama pull {model}", f"Installation {model}")
        
        if success:
            print_step(f"Mod√®le {model} install√© avec succ√®s", "OK")
            return True
        else:
            print_step(f"√âchec installation {model}: {output[:100]}", "WARNING")
    
    print_step("√âchec de l'installation de tous les mod√®les LLaVA", "ERROR")
    return False

def test_llava_generation():
    """Test rapide de g√©n√©ration avec LLaVA"""
    print_step("Test de g√©n√©ration avec LLaVA", "INFO")
    
    try:
        # R√©cup√©rer le premier mod√®le LLaVA disponible
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        data = response.json()
        models = [m['name'] for m in data.get('models', []) if 'llava' in m['name'].lower()]
        
        if not models:
            print_step("Aucun mod√®le LLaVA pour le test", "ERROR")
            return False
        
        model_name = models[0]
        print_step(f"Test avec le mod√®le: {model_name}", "INFO")
        
        payload = {
            "model": model_name,
            "messages": [{"role": "user", "content": "Say 'LLAVA_TEST_SUCCESS' to confirm you work"}],
            "stream": False,
            "options": {"temperature": 0.1, "num_predict": 10}
        }
        
        response = requests.post(
            "http://localhost:11434/api/chat",
            json=payload,
            timeout=20
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result.get('message', {}).get('content', '')
            print_step(f"R√©ponse du mod√®le: '{content[:50]}'", "OK")
            
            if 'success' in content.lower() or 'test' in content.lower():
                print_step("Test de g√©n√©ration r√©ussi!", "OK")
                return True
            else:
                print_step("G√©n√©ration OK mais r√©ponse inattendue", "WARNING")
                return True
        else:
            print_step(f"Erreur g√©n√©ration: HTTP {response.status_code}", "ERROR")
            return False
            
    except Exception as e:
        print_step(f"Erreur test g√©n√©ration: {e}", "ERROR")
        return False

def main():
    print("üîß SCRIPT DE R√âPARATION OLLAMA + LLAVA")
    print("="*50)
    
    # √âtape 1: Arr√™ter Ollama proprement
    print("\n1Ô∏è‚É£ ARR√äT D'OLLAMA")
    kill_ollama()
    
    # √âtape 2: Red√©marrer Ollama
    print("\n2Ô∏è‚É£ RED√âMARRAGE D'OLLAMA")
    if not start_ollama():
        print_step("Impossible de d√©marrer Ollama", "ERROR")
        print("üí° Solutions manuelles:")
        print("   - V√©rifiez l'installation: ollama --version")
        print("   - Red√©marrez manuellement: ollama serve")
        sys.exit(1)
    
    # √âtape 3: V√©rifier les mod√®les LLaVA
    print("\n3Ô∏è‚É£ V√âRIFICATION MOD√àLES LLAVA")
    has_llava, llava_models = check_llava_model()
    
    if not has_llava:
        print("\n4Ô∏è‚É£ INSTALLATION LLAVA")
        if not install_llava():
            print_step("Installation LLaVA √©chou√©e", "ERROR")
            print("üí° Essayez manuellement: ollama pull llava:latest")
            sys.exit(1)
    else:
        print_step("Mod√®les LLaVA d√©j√† pr√©sents", "OK")
    
    # √âtape 4: Test final
    print("\n5Ô∏è‚É£ TEST FINAL")
    if test_llava_generation():
        print("\n" + "="*50)
        print("üéâ R√âPARATION TERMIN√âE AVEC SUCC√àS!")
        print("‚úÖ Ollama + LLaVA sont maintenant op√©rationnels")
        print("\nüöÄ Vous pouvez maintenant lancer votre chatbot:")
        print("   python app.py")
        print("="*50)
    else:
        print("\n" + "="*50)
        print("‚ùå PROBL√àME PERSISTANT")
        print("üí° Actions suppl√©mentaires recommand√©es:")
        print("   1. Red√©marrez votre syst√®me")
        print("   2. R√©installez Ollama compl√®tement")
        print("   3. V√©rifiez les logs: journalctl -f -u ollama")
        print("="*50)

if __name__ == "__main__":
    main()