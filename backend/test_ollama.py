#!/usr/bin/env python3
"""
Test rapide pour v√©rifier qu'Ollama fonctionne correctement avec llava
"""

import ollama
import json
import sys

def test_ollama_connection():
    """Test de base de la connexion Ollama"""
    print("üîç Test de connexion √† Ollama...")
    
    try:
        response = ollama.list()
        models = [m['name'] for m in response.get('models', [])]
        print(f"‚úÖ Mod√®les disponibles: {models}")
        return models
    except Exception as e:
        print(f"‚ùå Erreur connexion: {e}")
        return None

def test_llava_model(model_name):
    """Test sp√©cifique du mod√®le llava"""
    print(f"üß™ Test du mod√®le {model_name}...")
    
    try:
        # Test simple sans image
        response = ollama.chat(
            model=model_name,
            messages=[{
                'role': 'user',
                'content': 'Hello! Please respond with exactly "LLAVA_TEST_OK" to confirm you are working.'
            }],
            options={
                'temperature': 0.1,
                'num_predict': 10
            }
        )
        
        if response and 'message' in response and 'content' in response['message']:
            response_text = response['message']['content'].strip()
            print(f"‚úÖ R√©ponse re√ßue: '{response_text}'")
            
            # V√©rifier que la r√©ponse contient notre test
            if 'test' in response_text.lower() or 'ok' in response_text.lower():
                print("‚úÖ Mod√®le r√©pond correctement")
                return True
            else:
                print("‚ö†Ô∏è R√©ponse inattendue mais le mod√®le fonctionne")
                return True
        else:
            print("‚ùå R√©ponse invalide")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test mod√®le: {e}")
        return False

def test_llava_with_image():
    """Test avec une image encod√©e en base64 (petite image de test)"""
    print("üñºÔ∏è Test avec image...")
    
    # Petite image 1x1 pixel en base64 (PNG transparent)
    test_image_b64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="
    
    llava_models = [m for m in ollama.list()['models'] if 'llava' in m['name'].lower()]
    if not llava_models:
        print("‚ùå Aucun mod√®le llava disponible")
        return False
    
    model_name = llava_models[0]['name']
    
    try:
        response = ollama.chat(
            model=model_name,
            messages=[{
                'role': 'user',
                'content': 'What do you see in this image? Respond briefly.',
                'images': [test_image_b64]
            }],
            options={
                'temperature': 0.5,
                'num_predict': 50
            }
        )
        
        if response and 'message' in response:
            response_text = response['message']['content']
            print(f"‚úÖ R√©ponse avec image: '{response_text[:100]}'")
            return True
        else:
            print("‚ùå Pas de r√©ponse avec image")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test image: {e}")
        return False

def main():
    print("=" * 50)
    print("  TEST RAPIDE OLLAMA + LLAVA")
    print("=" * 50)
    
    # Test 1: Connexion de base
    models = test_ollama_connection()
    if not models:
        print("\n‚ùå √âCHEC: Ollama ne r√©pond pas")
        print("Solutions:")
        print("1. D√©marrez Ollama: ollama serve")
        print("2. V√©rifiez qu'il tourne sur le port 11434")
        sys.exit(1)
    
    # Test 2: Trouver llava
    llava_models = [m for m in models if 'llava' in m.lower()]
    if not llava_models:
        print("\n‚ùå √âCHEC: Aucun mod√®le llava trouv√©")
        print("Solution: ollama pull llava:latest")
        sys.exit(1)
    
    print(f"\nüìã Mod√®les llava disponibles: {llava_models}")
    
    # Test 3: Test du mod√®le llava
    success = test_llava_model(llava_models[0])
    if not success:
        print(f"\n‚ùå √âCHEC: Le mod√®le {llava_models[0]} ne r√©pond pas correctement")
        sys.exit(1)
    
    # Test 4: Test avec image
    image_success = test_llava_with_image()
    if not image_success:
        print("\n‚ö†Ô∏è WARNING: Probl√®me avec le traitement d'images")
        print("Le chatbot fonctionnera mais peut avoir des probl√®mes avec les images")
    
    print("\n" + "=" * 50)
    if success and image_success:
        print("üéâ TOUS LES TESTS R√âUSSIS!")
        print("Votre configuration Ollama + llava fonctionne parfaitement")
    elif success:
        print("‚úÖ TESTS DE BASE R√âUSSIS")
        print("Le chatbot fonctionnera (probl√®me mineur avec les images)")
    
    print("\nüöÄ Vous pouvez maintenant lancer votre chatbot:")
    print("   cd backend")
    print("   python start_app.py")
    print("=" * 50)

if __name__ == "__main__":
    main()