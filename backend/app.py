from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import ollama
import sys
import json
import os
from pathlib import Path
import time

# Import des modules RAG et memory (avec gestion d'erreur)
try:
    from rag.retriever import RagRetriever
    from rag.vector_db import VectorDB
    from rag.loader import DocumentLoader
    from memory.manager import MemoryManager
    RAG_AVAILABLE = True
    print("‚úÖ Modules RAG import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ö†Ô∏è  Modules RAG non disponibles: {e}")
    print("üí° Le chatbot fonctionnera sans RAG")
    RAG_AVAILABLE = False

app = Flask(__name__) 
CORS(app)

print("üöÄ D√©marrage du serveur Flask...")

# Initialisation des composants RAG
rag_retriever = None
memory_manager = None
document_loader = None
rag_initialized = False

def test_ollama_connection():
    """Test de connexion Ollama am√©lior√© avec retry"""
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            print(f"üîç Test de connexion √† Ollama (tentative {attempt + 1}/{max_retries})...")
            
            # Test de base avec timeout
            response = ollama.list()
            
            if not response or 'models' not in response:
                raise Exception("R√©ponse Ollama invalide")
                
            model_names = [model.get('name', '') for model in response.get('models', [])]
            print(f"‚úÖ Mod√®les disponibles: {model_names}")
            
            # V√©rifier llava sp√©cifiquement
            llava_models = [name for name in model_names if 'llava' in name.lower()]
            
            if not llava_models:
                print("‚ö†Ô∏è Aucun mod√®le llava trouv√©!")
                print("üí° Installez-le avec: ollama pull llava:latest")
                return False, []
            
            print(f"‚úÖ Mod√®les llava disponibles: {llava_models}")
            return True, llava_models
            
        except Exception as e:
            print(f"‚ùå Tentative {attempt + 1} √©chou√©e: {e}")
            if attempt < max_retries - 1:
                print(f"‚è≥ Nouvelle tentative dans {retry_delay} secondes...")
                time.sleep(retry_delay)
            else:
                print("‚ùå Toutes les tentatives de connexion ont √©chou√©")
                print("üí° V√©rifiez que Ollama est d√©marr√©: ollama serve")
                return False, []

def get_best_llava_model():
    """Trouve le meilleur mod√®le llava disponible"""
    try:
        response = ollama.list()
        model_names = [model.get('name', '') for model in response.get('models', [])]
        
        # Priorit√©s des mod√®les llava (du meilleur au moins bon)
        preferred_models = [
            'llava:latest',
            'llava:13b',
            'llava:7b',
            'llava'
        ]
        
        for preferred in preferred_models:
            for available in model_names:
                if preferred in available.lower():
                    return available
        
        # Fallback: n'importe quel mod√®le contenant "llava"
        for available in model_names:
            if 'llava' in available.lower():
                return available
                
        return None
    except Exception:
        return None

def initialize_rag_system():
    """Initialise le syst√®me RAG si possible"""
    global rag_retriever, memory_manager, document_loader, rag_initialized
    
    if not RAG_AVAILABLE:
        print("‚ö†Ô∏è  RAG non disponible - modules non import√©s")
        return False
    
    try:
        print("üìÑ Initialisation du syst√®me RAG...")
        
        # Cr√©er les dossiers n√©cessaires
        os.makedirs("data/documents", exist_ok=True)
        os.makedirs("data/vector_db", exist_ok=True)
        os.makedirs("data/memory", exist_ok=True)
        
        # Initialiser le document loader
        document_loader = DocumentLoader("data/documents")
        
        # Scanner les documents disponibles
        docs_info = document_loader.scan_documents()
        print(f"üìä Documents scann√©s:")
        print(f"   - Total fichiers: {docs_info['total_files']}")
        print(f"   - Fichiers support√©s: {docs_info['supported_files']}")
        print(f"   - Types support√©s: {', '.join(document_loader.get_supported_extensions())}")
        
        if docs_info['supported_files'] == 0:
            print("‚ö†Ô∏è  Aucun document support√© trouv√©")
            print("üí° Le syst√®me RAG sera initialis√© sans documents")
        
        # Initialiser les composants (m√™me sans documents)
        try:
            vector_db = VectorDB()
            # Charger les documents seulement s'ils existent
            if docs_info['supported_files'] > 0:
                vector_db.initialize()
                print("‚úÖ Base vectorielle initialis√©e avec documents")
            else:
                print("‚úÖ Base vectorielle initialis√©e (vide)")
            
            rag_retriever = RagRetriever()
            memory_manager = MemoryManager()
            
            rag_initialized = True
            print("‚úÖ Syst√®me RAG initialis√©!")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur initialisation composants RAG: {e}")
            print("üí° Le syst√®me fonctionnera sans RAG")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur RAG: {e}")
        return False

def test_model_response(model_name):
    """Test si un mod√®le r√©pond correctement"""
    try:
        print(f"üß™ Test du mod√®le {model_name}...")
        
        response = ollama.chat(
            model=model_name,
            messages=[{
                'role': 'user',
                'content': 'Hello, respond with just "OK" to confirm you work.'
            }],
            options={'temperature': 0.1}
        )
        
        if response and 'message' in response and 'content' in response['message']:
            print(f"‚úÖ Mod√®le {model_name} fonctionne!")
            return True
        else:
            print(f"‚ùå R√©ponse invalide du mod√®le {model_name}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test mod√®le {model_name}: {e}")
        return False

def enhance_prompt_with_rag(user_message):
    """Enrichit le prompt avec le contexte RAG"""
    if not rag_initialized or not rag_retriever or not user_message.strip():
        return user_message, False
    
    try:
        context = rag_retriever.search(user_message, k=3)
        if context.strip():
            enhanced_prompt = f"""Contexte bas√© sur les documents disponibles:
{context}

Question de l'utilisateur: {user_message}

R√©ponds en utilisant prioritairement les informations du contexte fourni."""
            return enhanced_prompt, True
        else:
            return user_message, False
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur recherche RAG: {e}")
        return user_message, False

@app.route('/')
def serve_html():
    """Servir le fichier HTML depuis le dossier parent"""
    try:
        # Le HTML est dans ../frontend par rapport au script backend/app.py
        return send_from_directory('../frontend', 'chatbot.html')
    except Exception as e:
        return f"Erreur: Impossible de charger chatbot.html - {e}", 404

@app.route('/api/status', methods=['GET'])
def status():
    """Endpoint de diagnostic complet"""
    try:
        # Test Ollama avec retry
        ollama_ok, llava_models = test_ollama_connection()
        
        # Informations sur les documents
        docs_info = {'total_files': 0, 'supported_files': 0, 'files_by_type': {}}
        if document_loader:
            try:
                docs_info = document_loader.scan_documents()
            except Exception as e:
                print(f"Erreur scan documents: {e}")
        
        return jsonify({
            'status': 'ok' if ollama_ok else 'warning',
            'ollama_connected': ollama_ok,
            'models_available': llava_models,
            'llava_ready': len(llava_models) > 0,
            'rag_available': RAG_AVAILABLE,
            'rag_initialized': rag_initialized,
            'documents': {
                'total_files': docs_info['total_files'],
                'supported_files': docs_info['supported_files'],
                'files_by_type': docs_info.get('files_by_type', {}),
                'supported_extensions': document_loader.get_supported_extensions() if document_loader else ['.pdf']
            }
        })
        
    except Exception as e:
        print(f"‚ùå Erreur status: {e}")
        return jsonify({
            'status': 'error',
            'ollama_connected': False,
            'rag_available': RAG_AVAILABLE,
            'rag_initialized': False,
            'error': str(e)
        }), 503

@app.route('/api/test', methods=['GET'])
def test_endpoint():
    """Endpoint de test basique"""
    return jsonify({
        'message': 'Server is running!',
        'status': 'ok',
        'timestamp': time.time()
    })

@app.route('/api/initialize-rag', methods=['POST'])
def initialize_rag_endpoint():
    """Endpoint pour initialiser/r√©initialiser le RAG"""
    success = initialize_rag_system()
    
    docs_info = {}
    if document_loader:
        try:
            docs_info = document_loader.scan_documents()
        except Exception as e:
            print(f"Erreur scan documents: {e}")
    
    return jsonify({
        "success": success,
        "rag_initialized": rag_initialized,
        "message": "RAG system initialized" if success else "RAG initialization failed",
        "documents_info": docs_info
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        print(f"üì® Nouvelle requ√™te chat")
        
        # Validation des donn√©es
        data = request.json
        if not data:
            return jsonify({'error': 'Donn√©es JSON manquantes'}), 400
            
        user_message = data.get('message', '').strip()
        image_b64 = data.get('image')
        
        if not user_message and not image_b64:
            return jsonify({'error': 'Message ou image requis'}), 400

        print(f"üí¨ Message: '{user_message[:50]}{'...' if len(user_message) > 50 else ''}'")
        print(f"üñºÔ∏è  Image: {'Oui' if image_b64 else 'Non'}")

        # Trouver le meilleur mod√®le llava disponible
        model_to_use = get_best_llava_model()
        if not model_to_use:
            return jsonify({
                'error': 'Aucun mod√®le llava disponible. Installez avec: ollama pull llava:latest'
            }), 503

        print(f"üéØ Utilisation du mod√®le: {model_to_use}")

        # Test rapide du mod√®le avant utilisation
        if not test_model_response(model_to_use):
            return jsonify({
                'error': f'Le mod√®le {model_to_use} ne r√©pond pas correctement'
            }), 503

        # Enrichissement avec RAG (seulement pour les messages texte sans image)
        rag_used = False
        if user_message and not image_b64:
            enhanced_message, rag_used = enhance_prompt_with_rag(user_message)
            if rag_used:
                print("üìö Message enrichi avec RAG")
        else:
            enhanced_message = user_message or "D√©cris cette image en d√©tail"

        # Pr√©paration du message pour Ollama
        messages = [{
            'role': 'user',
            'content': enhanced_message
        }]
        
        if image_b64:
            messages[0]['images'] = [image_b64]

        print("ü§ñ Appel √† Ollama...")
        
        try:
            response = ollama.chat(
                model=model_to_use,
                messages=messages,
                options={
                    'temperature': 0.7,
                    'num_predict': 512,  # Limiter la longueur de r√©ponse
                    'timeout': 60       # Timeout plus long
                }
            )
            
            if not response or 'message' not in response or 'content' not in response['message']:
                raise Exception("R√©ponse Ollama invalide")
            
            bot_response = response['message']['content'].strip()
            
            if not bot_response:
                bot_response = "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse appropri√©e."
            
            print(f"‚úÖ R√©ponse g√©n√©r√©e: {len(bot_response)} caract√®res")
            
            # Sauvegarder dans la m√©moire si disponible
            if memory_manager:
                try:
                    memory_manager.add_conversation(
                        user_message=user_message,
                        bot_response=bot_response,
                        metadata={
                            "rag_used": rag_used,
                            "has_image": bool(image_b64),
                            "model": model_to_use
                        }
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur sauvegarde m√©moire: {e}")
            
            return jsonify({
                'response': bot_response,
                'status': 'success',
                'model_used': model_to_use,
                'rag_used': rag_used
            })
            
        except Exception as e:
            print(f"‚ùå Erreur Ollama d√©taill√©e: {e}")
            error_msg = str(e)
            
            if "connection" in error_msg.lower():
                error_msg = "Impossible de se connecter √† Ollama. V√©rifiez qu'Ollama est d√©marr√©."
            elif "timeout" in error_msg.lower():
                error_msg = "Timeout: La g√©n√©ration a pris trop de temps."
            elif "model" in error_msg.lower():
                error_msg = f"Probl√®me avec le mod√®le {model_to_use}. Essayez de le r√©installer."
            
            return jsonify({
                'error': f'Erreur Ollama: {error_msg}'
            }), 503
            
    except Exception as e:
        print(f"‚ùå Erreur serveur: {e}")
        return jsonify({
            'error': f'Erreur serveur interne: {str(e)}'
        }), 500

if __name__ == '__main__':
    print("üîß Tests de d√©marrage...")
    
    # Test Ollama au d√©marrage
    ollama_ok, llava_models = test_ollama_connection()
    
    if ollama_ok and llava_models:
        print(f"‚úÖ Ollama op√©rationnel avec {len(llava_models)} mod√®les llava!")
        
        # Test rapide du premier mod√®le
        if test_model_response(llava_models[0]):
            print("‚úÖ Mod√®le test√© et fonctionnel!")
        else:
            print("‚ö†Ô∏è Probl√®me avec le mod√®le - le chatbot peut dysfonctionner")
    else:
        print("‚ùå Probl√®mes Ollama critiques d√©tect√©s!")
        print("üí° Solutions possibles:")
        print("   1. V√©rifiez qu'Ollama est d√©marr√©: ollama serve")
        print("   2. Installez llava: ollama pull llava:latest")
        print("   3. Red√©marrez Ollama si n√©cessaire")
    
    # Tentative d'initialisation RAG
    rag_ok = initialize_rag_system()
    if rag_ok:
        print("‚úÖ Syst√®me RAG op√©rationnel!")
    else:
        print("‚ö†Ô∏è  RAG non initialis√© (fonctionnement en mode simple)")
    
    print("\nüåê Serveur Flask d√©marr√© sur:")
    print("   - Frontend: http://localhost:5000")
    print("   - API Test: http://localhost:5000/api/test")
    print("   - API Status: http://localhost:5000/api/status")
    print("\nüîß Pour d√©boguer:")
    print("   1. Testez: http://localhost:5000/api/test")
    print("   2. Status: http://localhost:5000/api/status")
    print("   3. Si probl√®mes: V√©rifiez les logs ci-dessus")
    
    app.run(host='0.0.0.0', port=5000, debug=True)