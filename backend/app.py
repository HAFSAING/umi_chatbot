from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import ollama
import sys
import json
import os
from pathlib import Path
import time
import requests

# Import des modules RAG et memory (avec gestion d'erreur)
try:
    from rag.retriever import RagRetriever
    from rag.vector_db import VectorDB
    from rag.loader import DocumentLoader
    from memory.manager import MemoryManager
    RAG_AVAILABLE = True
    print("‚úÖ Modules RAG import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ö†Ô∏è  Modules RAG non disponibles: {e}")
    print("üí° Le chatbot fonctionnera sans RAG")
    RAG_AVAILABLE = False

app = Flask(__name__) 
CORS(app)

print("üöÄ D√©marrage du serveur Flask...")

# Initialisation des composants RAG
rag_retriever = None
memory_manager = None
document_loader = None
rag_initialized = False

def test_ollama_connection():
    """Test de connexion Ollama am√©lior√© avec retry et API REST"""
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            print(f"üîç Test de connexion √† Ollama (tentative {attempt + 1}/{max_retries})...")
            
            # Test avec API REST directement (plus fiable)
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            
            if response.status_code != 200:
                raise Exception(f"Ollama API HTTP {response.status_code}")
            
            data = response.json()
            models = data.get('models', [])
            model_names = [model.get('name', '') for model in models]
            
            print(f"‚úÖ Mod√®les disponibles: {model_names}")
            
            # V√©rifier llava sp√©cifiquement
            llava_models = [name for name in model_names if 'llava' in name.lower()]
            
            if not llava_models:
                print("‚ö†Ô∏è Aucun mod√®le llava trouv√©!")
                print("üí° Installez-le avec: ollama pull llava:latest")
                return False, []
            
            print(f"‚úÖ Mod√®les llava disponibles: {llava_models}")
            return True, llava_models
            
        except requests.exceptions.ConnectionError:
            print(f"‚ùå Tentative {attempt + 1} √©chou√©e: Ollama service non accessible")
        except requests.exceptions.Timeout:
            print(f"‚ùå Tentative {attempt + 1} √©chou√©e: Timeout")
        except Exception as e:
            print(f"‚ùå Tentative {attempt + 1} √©chou√©e: {e}")
        
        if attempt < max_retries - 1:
            print(f"‚è≥ Nouvelle tentative dans {retry_delay} secondes...")
            time.sleep(retry_delay)
    
    print("‚ùå Toutes les tentatives de connexion ont √©chou√©")
    print("üí° V√©rifiez que Ollama est d√©marr√©: ollama serve")
    return False, []

def get_best_llava_model():
    """Trouve le meilleur mod√®le llava disponible"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            return None
            
        data = response.json()
        model_names = [model.get('name', '') for model in data.get('models', [])]
        
        # Priorit√©s des mod√®les llava (du meilleur au moins bon)
        preferred_models = [
            'llava:latest',
            'llava:13b',
            'llava:7b',
            'llava:34b',
            'llava'
        ]
        
        for preferred in preferred_models:
            for available in model_names:
                if preferred.lower() == available.lower():
                    return available
        
        # Fallback: n'importe quel mod√®le contenant "llava"
        for available in model_names:
            if 'llava' in available.lower():
                return available
                
        return None
    except Exception as e:
        print(f"Erreur get_best_llava_model: {e}")
        return None

def initialize_rag_system():
    """Initialise le syst√®me RAG si possible"""
    global rag_retriever, memory_manager, document_loader, rag_initialized
    
    if not RAG_AVAILABLE:
        print("‚ö†Ô∏è  RAG non disponible - modules non import√©s")
        return False
    
    try:
        print("üìÑ Initialisation du syst√®me RAG...")
        
        # Cr√©er les dossiers n√©cessaires
        os.makedirs("data/documents", exist_ok=True)
        os.makedirs("data/vector_db", exist_ok=True)
        os.makedirs("data/memory", exist_ok=True)
        
        # Initialiser le document loader
        document_loader = DocumentLoader("data/documents")
        
        # Scanner les documents disponibles
        docs_info = document_loader.scan_documents()
        print(f"üìä Documents scann√©s:")
        print(f"   - Total fichiers: {docs_info['total_files']}")
        print(f"   - Fichiers support√©s: {docs_info['supported_files']}")
        print(f"   - Types support√©s: {', '.join(document_loader.get_supported_extensions())}")
        
        if docs_info['supported_files'] == 0:
            print("‚ö†Ô∏è  Aucun document support√© trouv√©")
            print("üí° Le syst√®me RAG sera initialis√© sans documents")
        
        # Initialiser les composants (m√™me sans documents)
        try:
            vector_db = VectorDB()
            # Charger les documents seulement s'ils existent
            if docs_info['supported_files'] > 0:
                vector_db.initialize()
                print("‚úÖ Base vectorielle initialis√©e avec documents")
            else:
                print("‚úÖ Base vectorielle initialis√©e (vide)")
            
            rag_retriever = RagRetriever()
            memory_manager = MemoryManager()
            
            rag_initialized = True
            print("‚úÖ Syst√®me RAG initialis√©!")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur initialisation composants RAG: {e}")
            print("üí° Le syst√®me fonctionnera sans RAG")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur RAG: {e}")
        return False

def test_model_response(model_name):
    """Test si un mod√®le r√©pond correctement via API REST"""
    try:
        print(f"üß™ Test du mod√®le {model_name}...")
        
        payload = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "Hello, respond with just 'OK' to confirm you work."}
            ],
            "stream": False,
            "options": {"temperature": 0.1, "num_predict": 10}
        }
        
        response = requests.post(
            "http://localhost:11434/api/chat",
            json=payload,
            timeout=20
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result.get('message', {}).get('content', '')
            print(f"‚úÖ Mod√®le {model_name} fonctionne! R√©ponse: '{content[:30]}'")
            return True
        else:
            print(f"‚ùå Erreur test mod√®le {model_name}: HTTP {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test mod√®le {model_name}: {e}")
        return False

def enhance_prompt_with_rag(user_message):
    """Enrichit le prompt avec le contexte RAG"""
    if not rag_initialized or not rag_retriever or not user_message.strip():
        return user_message, False
    
    try:
        context = rag_retriever.search(user_message, k=3)
        if context.strip():
            enhanced_prompt = f"""Contexte bas√© sur les documents disponibles:
{context}

Question de l'utilisateur: {user_message}

R√©ponds en utilisant prioritairement les informations du contexte fourni."""
            return enhanced_prompt, True
        else:
            return user_message, False
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur recherche RAG: {e}")
        return user_message, False

@app.route('/')
def serve_html():
    """Servir le fichier HTML depuis le dossier frontend"""
    try:
        return send_from_directory('../frontend', 'chatbot.html')
    except Exception as e:
        return f"Erreur: Impossible de charger chatbot.html - {e}", 404

@app.route('/api/status', methods=['GET'])
def status():
    """Endpoint de diagnostic complet"""
    try:
        # Test Ollama avec retry
        ollama_ok, llava_models = test_ollama_connection()
        
        # Informations sur les documents
        docs_info = {'total_files': 0, 'supported_files': 0, 'files_by_type': {}}
        if document_loader:
            try:
                docs_info = document_loader.scan_documents()
            except Exception as e:
                print(f"Erreur scan documents: {e}")
        
        return jsonify({
            'status': 'ok' if ollama_ok else 'warning',
            'ollama_connected': ollama_ok,
            'models_available': llava_models,
            'llava_ready': len(llava_models) > 0,
            'rag_available': RAG_AVAILABLE,
            'rag_initialized': rag_initialized,
            'documents': {
                'total_files': docs_info['total_files'],
                'supported_files': docs_info['supported_files'],
                'files_by_type': docs_info.get('files_by_type', {}),
                'supported_extensions': document_loader.get_supported_extensions() if document_loader else ['.pdf']
            },
            'server_info': {
                'python_version': sys.version,
                'working_directory': str(Path.cwd())
            }
        })
        
    except Exception as e:
        print(f"‚ùå Erreur status: {e}")
        return jsonify({
            'status': 'error',
            'ollama_connected': False,
            'rag_available': RAG_AVAILABLE,
            'rag_initialized': False,
            'error': str(e)
        }), 503

@app.route('/api/test', methods=['GET'])
def test_endpoint():
    """Endpoint de test basique"""
    return jsonify({
        'message': 'Server is running!',
        'status': 'ok',
        'timestamp': time.time()
    })

@app.route('/api/initialize-rag', methods=['POST'])
def initialize_rag_endpoint():
    """Endpoint pour initialiser/r√©initialiser le RAG"""
    success = initialize_rag_system()
    
    docs_info = {}
    if document_loader:
        try:
            docs_info = document_loader.scan_documents()
        except Exception as e:
            print(f"Erreur scan documents: {e}")
    
    return jsonify({
        "success": success,
        "rag_initialized": rag_initialized,
        "message": "RAG system initialized" if success else "RAG initialization failed",
        "documents_info": docs_info
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        print(f"üì® Nouvelle requ√™te chat")
        
        # Validation des donn√©es
        data = request.json
        if not data:
            return jsonify({'error': 'Donn√©es JSON manquantes'}), 400
            
        user_message = data.get('message', '').strip()
        image_b64 = data.get('image')
        
        if not user_message and not image_b64:
            return jsonify({'error': 'Message ou image requis'}), 400

        print(f"üí¨ Message: '{user_message[:50]}{'...' if len(user_message) > 50 else ''}'")
        print(f"üñºÔ∏è  Image: {'Oui' if image_b64 else 'Non'}")

        # Trouver le meilleur mod√®le llava disponible
        model_to_use = get_best_llava_model()
        if not model_to_use:
            return jsonify({
                'error': 'Aucun mod√®le llava disponible. V√©rifiez: ollama list | grep llava'
            }), 503

        print(f"üéØ Utilisation du mod√®le: {model_to_use}")

        # Test rapide du mod√®le avant utilisation
        if not test_model_response(model_to_use):
            return jsonify({
                'error': f'Le mod√®le {model_to_use} ne r√©pond pas correctement. Red√©marrez Ollama.'
            }), 503

        # Enrichissement avec RAG (seulement pour les messages texte sans image)
        rag_used = False
        if user_message and not image_b64:
            enhanced_message, rag_used = enhance_prompt_with_rag(user_message)
            if rag_used:
                print("üìö Message enrichi avec RAG")
        else:
            enhanced_message = user_message or "D√©cris cette image en d√©tail"

        # Pr√©paration du payload pour API REST
        payload = {
            "model": model_to_use,
            "messages": [
                {
                    "role": "user",
                    "content": enhanced_message
                }
            ],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 512,
                "num_ctx": 2048
            }
        }
        
        if image_b64:
            payload["messages"][0]["images"] = [image_b64]

        print("ü§ñ Appel √† Ollama via API REST...")
        
        try:
            response = requests.post(
                "http://localhost:11434/api/chat",
                json=payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.text[:300] if response.text else "Pas de d√©tails"
                raise Exception(f"Ollama API HTTP {response.status_code}: {error_detail}")
            
            result = response.json()
            
            if 'message' not in result or 'content' not in result['message']:
                raise Exception("R√©ponse Ollama invalide - structure inattendue")
            
            bot_response = result['message']['content'].strip()
            
            if not bot_response:
                bot_response = "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse appropri√©e."
            
            print(f"‚úÖ R√©ponse g√©n√©r√©e: {len(bot_response)} caract√®res")
            
            # Sauvegarder dans la m√©moire si disponible
            if memory_manager:
                try:
                    memory_manager.add_conversation(
                        user_message=user_message,
                        bot_response=bot_response,
                        metadata={
                            "rag_used": rag_used,
                            "has_image": bool(image_b64),
                            "model": model_to_use
                        }
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur sauvegarde m√©moire: {e}")
            
            return jsonify({
                'response': bot_response,
                'status': 'success',
                'model_used': model_to_use,
                'rag_used': rag_used
            })
            
        except requests.exceptions.Timeout:
            print("‚ùå Timeout Ollama (>60s)")
            return jsonify({
                'error': 'Timeout: La g√©n√©ration a pris trop de temps. Essayez avec un message plus court.'
            }), 503
        except requests.exceptions.ConnectionError:
            print("‚ùå Connexion Ollama impossible")
            return jsonify({
                'error': 'Impossible de se connecter √† Ollama. V√©rifiez qu\'Ollama est d√©marr√©: ollama serve'
            }), 503
        except Exception as e:
            print(f"‚ùå Erreur Ollama d√©taill√©e: {e}")
            error_msg = str(e)
            
            if "connection" in error_msg.lower():
                error_msg = "Impossible de se connecter √† Ollama. V√©rifiez qu'Ollama est d√©marr√©."
            elif "timeout" in error_msg.lower():
                error_msg = "Timeout: La g√©n√©ration a pris trop de temps."
            elif "model" in error_msg.lower():
                error_msg = f"Probl√®me avec le mod√®le {model_to_use}. Essayez de le r√©installer."
            elif "503" in error_msg or "service" in error_msg.lower():
                error_msg = "Service Ollama indisponible. Red√©marrez Ollama."
            
            return jsonify({
                'error': f'Erreur Ollama: {error_msg}'
            }), 503
            
    except Exception as e:
        print(f"‚ùå Erreur serveur: {e}")
        return jsonify({
            'error': f'Erreur serveur interne: {str(e)}'
        }), 500

@app.route('/api/debug/ollama', methods=['GET'])
def debug_ollama():
    """Endpoint de debug pour Ollama"""
    try:
        debug_info = {}
        
        # Test connexion basique
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            if response.status_code == 200:
                debug_info['ollama_version'] = response.json()
                debug_info['ollama_accessible'] = True
            else:
                debug_info['ollama_accessible'] = False
                debug_info['ollama_error'] = f"HTTP {response.status_code}"
        except Exception as e:
            debug_info['ollama_accessible'] = False
            debug_info['ollama_error'] = str(e)
        
        # Liste des mod√®les
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                debug_info['models'] = models_data.get('models', [])
                debug_info['llava_models'] = [
                    m['name'] for m in debug_info['models'] 
                    if 'llava' in m['name'].lower()
                ]
            else:
                debug_info['models_error'] = f"HTTP {response.status_code}"
        except Exception as e:
            debug_info['models_error'] = str(e)
        
        # Test de g√©n√©ration simple
        llava_models = debug_info.get('llava_models', [])
        if llava_models:
            try:
                payload = {
                    "model": llava_models[0],
                    "messages": [{"role": "user", "content": "Say 'TEST_OK'"}],
                    "stream": False,
                    "options": {"temperature": 0.1, "num_predict": 5}
                }
                
                response = requests.post(
                    "http://localhost:11434/api/chat",
                    json=payload,
                    timeout=15
                )
                
                if response.status_code == 200:
                    result = response.json()
                    debug_info['generation_test'] = {
                        'success': True,
                        'response': result.get('message', {}).get('content', '')[:100]
                    }
                else:
                    debug_info['generation_test'] = {
                        'success': False,
                        'error': f"HTTP {response.status_code}"
                    }
            except Exception as e:
                debug_info['generation_test'] = {
                    'success': False,
                    'error': str(e)
                }
        
        return jsonify(debug_info)
        
    except Exception as e:
        return jsonify({'debug_error': str(e)}), 500

if __name__ == '__main__':
    print("üîß Tests de d√©marrage...")
    
    # Test Ollama au d√©marrage
    ollama_ok, llava_models = test_ollama_connection()
    
    if ollama_ok and llava_models:
        print(f"‚úÖ Ollama op√©rationnel avec {len(llava_models)} mod√®les llava!")
        
        # Test rapide du premier mod√®le
        if test_model_response(llava_models[0]):
            print("‚úÖ Mod√®le test√© et fonctionnel!")
        else:
            print("‚ö†Ô∏è Probl√®me avec le mod√®le - le chatbot peut dysfonctionner")
    else:
        print("‚ùå Probl√®mes Ollama critiques d√©tect√©s!")
        print("üí° Solutions possibles:")
        print("   1. V√©rifiez qu'Ollama est d√©marr√©: ollama serve")
        print("   2. Installez llava: ollama pull llava:latest")
        print("   3. Red√©marrez Ollama si n√©cessaire")
        print("   4. Testez manuellement: ollama run llava:latest 'hello'")
    
    # Tentative d'initialisation RAG
    rag_ok = initialize_rag_system()
    if rag_ok:
        print("‚úÖ Syst√®me RAG op√©rationnel!")
    else:
        print("‚ö†Ô∏è  RAG non initialis√© (fonctionnement en mode simple)")
    
    print("\nüåê Serveur Flask d√©marr√© sur:")
    print("   - Frontend: http://localhost:5000")
    print("   - API Test: http://localhost:5000/api/test")
    print("   - API Status: http://localhost:5000/api/status")
    print("   - Debug Ollama: http://localhost:5000/api/debug/ollama")
    print("\nüîß Pour d√©boguer:")
    print("   1. Testez: http://localhost:5000/api/test")
    print("   2. Status: http://localhost:5000/api/status")  
    print("   3. Debug Ollama: http://localhost:5000/api/debug/ollama")
    print("   4. Si probl√®mes: V√©rifiez les logs ci-dessus")
    
    app.run(host='0.0.0.0', port=5000, debug=True)